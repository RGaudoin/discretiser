{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Policy Training\n",
    "\n",
    "Train DQN to learn service policies and compare against baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.scenarios.basic_bathtub import BasicBathtubScenario\n",
    "from src.policy import LinearIntervalPolicy, FixedIntervalPolicy, NoOpPolicy\n",
    "from src.runner import run_scenario, compare_policies\n",
    "from src.rl import ServiceEnv, train_dqn, evaluate_model, DQNPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same scenario as policy_optimisation notebook\n",
    "scenario = BasicBathtubScenario(\n",
    "    scale1=100.0,\n",
    "    scale2=200.0,\n",
    "    service_cost=0.5,\n",
    "    failure_cost=150.0,\n",
    "    revenue_per_time=1.50,\n",
    ")\n",
    "\n",
    "# Print key parameters for reference\n",
    "print(\"Scenario parameters:\")\n",
    "print(f\"  Bathtub shape: shape1={scenario.failure_model.shape1}, shape2={scenario.failure_model.shape2}\")\n",
    "print(f\"  Scales: scale1={scenario.failure_model.scale1}, scale2={scenario.failure_model.scale2}\")\n",
    "print(f\"  Delta_t (age reduction per service): {scenario.failure_model.delta_t}\")\n",
    "print(f\"  Costs: service={scenario.costs.service_cost}, failure={scenario.costs.failure_cost}\")\n",
    "print(f\"  Revenue per time: {scenario.costs.revenue_per_time}\")\n",
    "\n",
    "MAX_TIME = 150.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Comparison\n",
    "\n",
    "First, establish baseline performance with known policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline policies including optimised from policy_optimisation notebook\n",
    "# Best from multi-start: c=48.4, r=0.50 -> a=24.2, b=24.2\n",
    "policies = {\n",
    "    'no_service': NoOpPolicy(),\n",
    "    'fixed_25': FixedIntervalPolicy(interval=25.0),\n",
    "    'fixed_50': FixedIntervalPolicy(interval=50.0),\n",
    "    'linear_15_10': LinearIntervalPolicy(a=15.0, b=10.0),  # baseline from policy_opt\n",
    "    'optimised_linear': LinearIntervalPolicy(a=24.2, b=24.2),  # best from grid+NM\n",
    "}\n",
    "\n",
    "# Compare policies\n",
    "results = compare_policies(\n",
    "    scenario,\n",
    "    policies,\n",
    "    n_subjects=2000,\n",
    "    max_time=MAX_TIME,\n",
    "    n_repeats=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Baseline Policy Comparison\")\n",
    "print(\"=\" * 50)\n",
    "for name, stats in sorted(results.items(), key=lambda x: -x[1]['mean']):\n",
    "    print(f\"{name:20s}: mean={stats['mean']:8.2f} Â± {stats['std']:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DQN\n",
    "# Action space now: [0.1, 5, 10, 15, ..., 100, inf] (22 actions)\n",
    "print(f\"Action space: {len(ServiceEnv.ACTION_DELAYS)} actions\")\n",
    "print(f\"Delays: {ServiceEnv.ACTION_DELAYS[:5]} ... {ServiceEnv.ACTION_DELAYS[-3:]}\")\n",
    "\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Network architecture - toggle activation:\n",
    "USE_TANH = False  # Set True for Tanh, False for ReLU\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    #net_arch=[128, 128],\n",
    "    net_arch=[256, 128, 64],\n",
    "    activation_fn=nn.Tanh if USE_TANH else nn.ReLU,\n",
    ")\n",
    "\n",
    "# Create environment\n",
    "env = ServiceEnv(scenario, max_time=MAX_TIME, seed=42)\n",
    "env = Monitor(env)\n",
    "\n",
    "# TensorBoard: run `tensorboard --logdir ./logs` to watch\n",
    "print(f\"\\nNetwork: {policy_kwargs}\")\n",
    "print(\"Training DQN...\")\n",
    "\n",
    "model = DQN(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=50_000,\n",
    "    batch_size=128,\n",
    "    gamma=0.99,\n",
    "    exploration_fraction=0.5,\n",
    "    exploration_final_eps=0.05,\n",
    "    target_update_interval=500,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    seed=42,\n",
    "    tensorboard_log=\"./logs\",\n",
    ")\n",
    "\n",
    "# Callbacks for logging\n",
    "from src.rl.training import ActionStatsCallback, RewardLoggerCallback\n",
    "reward_logger = RewardLoggerCallback()\n",
    "callbacks = [reward_logger, ActionStatsCallback(log_freq=1000)]\n",
    "\n",
    "model.learn(total_timesteps=200_000, callback=callbacks)\n",
    "\n",
    "# Store for later cells\n",
    "training_rewards = reward_logger.episode_rewards\n",
    "trained_model = model\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Episodes: {len(training_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# Smooth rewards with rolling average\n",
    "rewards = training_rewards  # From previous cell\n",
    "window = 50\n",
    "if len(rewards) > window:\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(smoothed, label=f'Rolling avg (window={window})')\n",
    "    ax.scatter(range(len(rewards)), rewards, alpha=0.1, s=1, label='Raw')\n",
    "else:\n",
    "    ax.plot(rewards)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Cumulative Reward')\n",
    "ax.set_title('DQN Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained model\n",
    "dqn_rewards = evaluate_model(\n",
    "    trained_model,\n",
    "    scenario,\n",
    "    n_episodes=500,\n",
    "    max_time=MAX_TIME,\n",
    "    seed=42,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"DQN Performance (500 episodes):\")\n",
    "print(f\"  Mean reward: {np.mean(dqn_rewards):.2f}\")\n",
    "print(f\"  Std reward:  {np.std(dqn_rewards):.2f}\")\n",
    "print(f\"  Min reward:  {np.min(dqn_rewards):.2f}\")\n",
    "print(f\"  Max reward:  {np.max(dqn_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DQN vs baselines\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Collect all results\n",
    "all_results = {name: stats['mean'] for name, stats in results.items()}\n",
    "all_results['DQN'] = np.mean(dqn_rewards)\n",
    "\n",
    "# Sort by performance\n",
    "sorted_results = sorted(all_results.items(), key=lambda x: x[1])\n",
    "names = [r[0] for r in sorted_results]\n",
    "means = [r[1] for r in sorted_results]\n",
    "\n",
    "colors = ['green' if n == 'DQN' else 'steelblue' for n in names]\n",
    "bars = ax.barh(names, means, color=colors)\n",
    "\n",
    "ax.set_xlabel('Mean Reward')\n",
    "ax.set_title('DQN vs Baseline Policies')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean in zip(bars, means):\n",
    "    ax.text(mean + 5, bar.get_y() + bar.get_height()/2, \n",
    "            f'{mean:.1f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Diagnostic: Run a few episodes manually\n",
    "print(\"\\n--- Diagnostic: Episode traces ---\")\n",
    "print(\"Obs (normalised): [time, last_int, svc_cnt, avg_int, dur] - shown denormalised below\")\n",
    "env = ServiceEnv(scenario, max_time=MAX_TIME, seed=123)\n",
    "for ep in range(3):\n",
    "    obs, info = env.reset(seed=123 + ep)\n",
    "    print(f\"\\nEpisode {ep+1}: durability={obs[4]*10:.2f}\")\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        action, _ = trained_model.predict(obs, deterministic=True)\n",
    "        delay = ServiceEnv.ACTION_DELAYS[int(action)]\n",
    "        obs, reward, term, trunc, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        if step <= 5:\n",
    "            t = obs[0] * MAX_TIME\n",
    "            last_int = obs[1] * MAX_TIME\n",
    "            print(f\"  step {step}: t={t:5.1f}, last_int={last_int:5.1f}, action={int(action)} (delay={delay:4.0f}), reward={reward:7.1f}\")\n",
    "        if term or trunc:\n",
    "            status = \"FAILED\" if term else \"TRUNCATED\"\n",
    "            print(f\"  ... {status} at t={info['time']:.1f}, services={info['service_count']}, total={total_reward:.1f}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse DQN Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse action distribution across different states\n",
    "from src.rl.environment import ServiceEnv\n",
    "\n",
    "# Sample states and get actions\n",
    "env = ServiceEnv(scenario, max_time=MAX_TIME, seed=42)\n",
    "\n",
    "# Collect state -> action pairs\n",
    "# Obs (normalised): [current_time, last_interval, service_count, avg_interval, durability]\n",
    "state_actions = []\n",
    "for ep in range(100):\n",
    "    obs, _ = env.reset(seed=42 + ep)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = trained_model.predict(obs, deterministic=True)\n",
    "        state_actions.append({\n",
    "            'current_time': obs[0] * MAX_TIME,      # denormalise\n",
    "            'last_interval': obs[1] * MAX_TIME,\n",
    "            'service_count': obs[2] * 100,\n",
    "            'avg_interval': obs[3] * MAX_TIME,\n",
    "            'durability': obs[4] * 10,\n",
    "            'action': int(action),\n",
    "            'delay': ServiceEnv.ACTION_DELAYS[int(action)]\n",
    "        })\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(state_actions)\n",
    "\n",
    "print(\"Action distribution:\")\n",
    "print(df['delay'].value_counts().sort_index())\n",
    "print(f\"\\nTotal decisions: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot action choices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Action vs last_interval (colored by durability)\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(df['last_interval'], df['delay'], c=df['durability'], \n",
    "                     alpha=0.5, s=20, cmap='viridis')\n",
    "ax.set_xlabel('Last Interval')\n",
    "ax.set_ylabel('Chosen Delay')\n",
    "ax.set_title('Action vs Last Interval')\n",
    "plt.colorbar(scatter, ax=ax, label='Durability')\n",
    "\n",
    "# Action vs durability (colored by last_interval)\n",
    "ax = axes[1]\n",
    "for action_idx in range(len(ServiceEnv.ACTION_DELAYS)):\n",
    "    mask = df['action'] == action_idx\n",
    "    if mask.any():\n",
    "        delay = ServiceEnv.ACTION_DELAYS[action_idx]\n",
    "        label = f'delay={delay}' if delay < float('inf') else 'no service'\n",
    "        ax.scatter(df.loc[mask, 'durability'], df.loc[mask, 'last_interval'],\n",
    "                   alpha=0.3, s=10, label=label)\n",
    "ax.set_xlabel('Durability')\n",
    "ax.set_ylabel('Last Interval')\n",
    "ax.set_title('Action by Durability & Last Interval')\n",
    "ax.legend(markerscale=3, fontsize=8)\n",
    "\n",
    "# Average delay vs durability bins\n",
    "ax = axes[2]\n",
    "df['durability_bin'] = pd.cut(df['durability'], bins=8)\n",
    "finite_delays = df[df['delay'] < float('inf')]\n",
    "if len(finite_delays) > 0:\n",
    "    avg_delay = finite_delays.groupby('durability_bin', observed=True)['delay'].mean()\n",
    "    avg_delay.plot(kind='bar', ax=ax, color='steelblue')\n",
    "    ax.set_xlabel('Durability Bin')\n",
    "    ax.set_ylabel('Average Delay')\n",
    "    ax.set_title('Avg Delay by Durability')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# What interval does optimal linear use?\n",
    "print(\"\\nOptimal linear policy intervals for reference:\")\n",
    "print(\"  interval = 24.2 + 24.2 * durability\")\n",
    "for d in [0.7, 1.0, 1.3, 1.6]:\n",
    "    print(f\"  durability={d:.1f} -> interval={24.2 + 24.2*d:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "model_path = '../models/dqn_service_policy'\n",
    "trained_model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2510",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
