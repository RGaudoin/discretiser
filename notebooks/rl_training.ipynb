{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Policy Training\n",
    "\n",
    "Train DQN to learn service policies and compare against baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.scenarios.basic_bathtub import BasicBathtubScenario\n",
    "from src.policy import LinearIntervalPolicy, FixedIntervalPolicy, NoOpPolicy\n",
    "from src.runner import run_scenario, compare_policies\n",
    "from src.rl import ServiceEnv, train_dqn, evaluate_model, DQNPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use same scenario as policy_optimisation notebook\nscenario = BasicBathtubScenario(\n    scale1=100.0,\n    scale2=200.0,\n    service_cost=0.5,\n    failure_cost=150.0,\n    revenue_per_time=1.50,\n)\n\n# Print key parameters for reference\nprint(\"Scenario parameters:\")\nprint(f\"  Bathtub shape: shape1={scenario.failure_model.shape1}, shape2={scenario.failure_model.shape2}\")\nprint(f\"  Scales: scale1={scenario.failure_model.scale1}, scale2={scenario.failure_model.scale2}\")\nprint(f\"  Delta_t (age reduction per service): {scenario.failure_model.delta_t}\")\nprint(f\"  Costs: service={scenario.costs.service_cost}, failure={scenario.costs.failure_cost}\")\nprint(f\"  Revenue per time: {scenario.costs.revenue_per_time}\")\n\nMAX_TIME = 150.0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Comparison\n",
    "\n",
    "First, establish baseline performance with known policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define baseline policies including optimised from policy_optimisation notebook\n# Best from multi-start: c=48.4, r=0.50 -> a=24.2, b=24.2\npolicies = {\n    'no_service': NoOpPolicy(),\n    'fixed_25': FixedIntervalPolicy(interval=25.0),\n    'fixed_50': FixedIntervalPolicy(interval=50.0),\n    'linear_15_10': LinearIntervalPolicy(a=15.0, b=10.0),  # baseline from policy_opt\n    'optimised_linear': LinearIntervalPolicy(a=24.2, b=24.2),  # best from grid+NM\n}\n\n# Compare policies\nresults = compare_policies(\n    scenario,\n    policies,\n    n_subjects=2000,\n    max_time=MAX_TIME,\n    n_repeats=5,\n    seed=42\n)\n\n# Display results\nprint(\"Baseline Policy Comparison\")\nprint(\"=\" * 50)\nfor name, stats in sorted(results.items(), key=lambda x: -x[1]['mean']):\n    print(f\"{name:20s}: mean={stats['mean']:8.2f} Â± {stats['std']:6.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train DQN\nprint(\"Training DQN...\")\ntraining_result = train_dqn(\n    scenario,\n    total_timesteps=200_000,  # More steps for harder problem\n    learning_rate=1e-4,\n    buffer_size=50_000,\n    gamma=0.99,\n    exploration_fraction=0.3,\n    exploration_final_eps=0.05,\n    max_time=MAX_TIME,\n    seed=42,\n    verbose=1,\n)\n\nprint(f\"\\nTraining complete!\")\nprint(f\"Final mean reward: {training_result.final_mean_reward:.2f}\")\nprint(f\"Episodes during training: {len(training_result.training_rewards)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# Smooth rewards with rolling average\n",
    "rewards = training_result.training_rewards\n",
    "window = 50\n",
    "if len(rewards) > window:\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(smoothed, label=f'Rolling avg (window={window})')\n",
    "    ax.scatter(range(len(rewards)), rewards, alpha=0.1, s=1, label='Raw')\n",
    "else:\n",
    "    ax.plot(rewards)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Cumulative Reward')\n",
    "ax.set_title('DQN Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate trained model\ndqn_rewards = evaluate_model(\n    training_result.model,\n    scenario,\n    n_episodes=500,\n    max_time=MAX_TIME,\n    seed=42,\n    deterministic=True\n)\n\nprint(f\"DQN Performance (500 episodes):\")\nprint(f\"  Mean reward: {np.mean(dqn_rewards):.2f}\")\nprint(f\"  Std reward:  {np.std(dqn_rewards):.2f}\")\nprint(f\"  Min reward:  {np.min(dqn_rewards):.2f}\")\nprint(f\"  Max reward:  {np.max(dqn_rewards):.2f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare DQN vs baselines\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Collect all results\nall_results = {name: stats['mean'] for name, stats in results.items()}\nall_results['DQN'] = np.mean(dqn_rewards)\n\n# Sort by performance\nsorted_results = sorted(all_results.items(), key=lambda x: x[1])\nnames = [r[0] for r in sorted_results]\nmeans = [r[1] for r in sorted_results]\n\ncolors = ['green' if n == 'DQN' else 'steelblue' for n in names]\nbars = ax.barh(names, means, color=colors)\n\nax.set_xlabel('Mean Reward')\nax.set_title('DQN vs Baseline Policies')\nax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n\n# Add value labels\nfor bar, mean in zip(bars, means):\n    ax.text(mean + 5, bar.get_y() + bar.get_height()/2, \n            f'{mean:.1f}', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# Diagnostic: Run a few episodes manually\nprint(\"\\n--- Diagnostic: Episode traces ---\")\nprint(\"Obs (normalised): [time, last_int, svc_cnt, avg_int, dur] - shown denormalised below\")\nenv = ServiceEnv(scenario, max_time=MAX_TIME, seed=123)\nfor ep in range(3):\n    obs, info = env.reset(seed=123 + ep)\n    print(f\"\\nEpisode {ep+1}: durability={obs[4]*10:.2f}\")\n    total_reward = 0\n    step = 0\n    while True:\n        action, _ = training_result.model.predict(obs, deterministic=True)\n        delay = ServiceEnv.ACTION_DELAYS[int(action)]\n        obs, reward, term, trunc, info = env.step(action)\n        total_reward += reward\n        step += 1\n        if step <= 5:\n            t = obs[0] * MAX_TIME\n            last_int = obs[1] * MAX_TIME\n            print(f\"  step {step}: t={t:5.1f}, last_int={last_int:5.1f}, action={int(action)} (delay={delay:4.0f}), reward={reward:7.1f}\")\n        if term or trunc:\n            status = \"FAILED\" if term else \"TRUNCATED\"\n            print(f\"  ... {status} at t={info['time']:.1f}, services={info['service_count']}, total={total_reward:.1f}\")\n            break"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse DQN Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyse action distribution across different states\nfrom src.rl.environment import ServiceEnv\n\n# Sample states and get actions\nenv = ServiceEnv(scenario, max_time=MAX_TIME, seed=42)\n\n# Collect state -> action pairs\n# Obs (normalised): [current_time, last_interval, service_count, avg_interval, durability]\nstate_actions = []\nfor ep in range(100):\n    obs, _ = env.reset(seed=42 + ep)\n    done = False\n    while not done:\n        action, _ = training_result.model.predict(obs, deterministic=True)\n        state_actions.append({\n            'current_time': obs[0] * MAX_TIME,      # denormalise\n            'last_interval': obs[1] * MAX_TIME,\n            'service_count': obs[2] * 100,\n            'avg_interval': obs[3] * MAX_TIME,\n            'durability': obs[4] * 10,\n            'action': int(action),\n            'delay': ServiceEnv.ACTION_DELAYS[int(action)]\n        })\n        obs, _, terminated, truncated, _ = env.step(action)\n        done = terminated or truncated\n\nimport pandas as pd\ndf = pd.DataFrame(state_actions)\n\nprint(\"Action distribution:\")\nprint(df['delay'].value_counts().sort_index())\nprint(f\"\\nTotal decisions: {len(df)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot action choices\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\n# Action vs last_interval (colored by durability)\nax = axes[0]\nscatter = ax.scatter(df['last_interval'], df['delay'], c=df['durability'], \n                     alpha=0.5, s=20, cmap='viridis')\nax.set_xlabel('Last Interval')\nax.set_ylabel('Chosen Delay')\nax.set_title('Action vs Last Interval')\nplt.colorbar(scatter, ax=ax, label='Durability')\n\n# Action vs durability (colored by last_interval)\nax = axes[1]\nfor action_idx in range(len(ServiceEnv.ACTION_DELAYS)):\n    mask = df['action'] == action_idx\n    if mask.any():\n        delay = ServiceEnv.ACTION_DELAYS[action_idx]\n        label = f'delay={delay}' if delay < float('inf') else 'no service'\n        ax.scatter(df.loc[mask, 'durability'], df.loc[mask, 'last_interval'],\n                   alpha=0.3, s=10, label=label)\nax.set_xlabel('Durability')\nax.set_ylabel('Last Interval')\nax.set_title('Action by Durability & Last Interval')\nax.legend(markerscale=3, fontsize=8)\n\n# Average delay vs durability bins\nax = axes[2]\ndf['durability_bin'] = pd.cut(df['durability'], bins=8)\nfinite_delays = df[df['delay'] < float('inf')]\nif len(finite_delays) > 0:\n    avg_delay = finite_delays.groupby('durability_bin', observed=True)['delay'].mean()\n    avg_delay.plot(kind='bar', ax=ax, color='steelblue')\n    ax.set_xlabel('Durability Bin')\n    ax.set_ylabel('Average Delay')\n    ax.set_title('Avg Delay by Durability')\n    plt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n# What interval does optimal linear use?\nprint(\"\\nOptimal linear policy intervals for reference:\")\nprint(\"  interval = 24.2 + 24.2 * durability\")\nfor d in [0.7, 1.0, 1.3, 1.6]:\n    print(f\"  durability={d:.1f} -> interval={24.2 + 24.2*d:.1f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "model_path = '../models/dqn_service_policy'\n",
    "training_result.model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2510",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}