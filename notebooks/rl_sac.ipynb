{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RL Policy Training - SAC\n\nTrain SAC (Soft Actor-Critic) with continuous action space to learn service policies.\n\nKey differences from DQN:\n- Continuous action space: agent outputs delay directly [0, max_delay]\n- Off-policy (like DQN) - efficient replay buffer usage\n- Entropy regularisation - helps exploration in noisy environments\n\nSee also:\n- `rl_quickstart.ipynb` - Minimal example (runnable by anyone)\n- `rl_dqn.ipynb` - DQN with discrete action space\n\n**Advanced algorithms:** For expected-value RL, surrogate models, and custom algorithms, contact for access to `discretiser-surrogate`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.rl import (\n",
    "    ServiceEnv,\n",
    "    get_default_scenario,\n",
    "    print_scenario_info,\n",
    "    compare_with_baselines,\n",
    "    run_sanity_check,\n",
    "    evaluate_model,\n",
    "    RewardLoggerCallback,\n",
    "    EpisodeDiagnosticsCallback,\n",
    "    ContinuousActionStatsCallback,\n",
    "    DEFAULT_MAX_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use shared scenario setup\nscenario = get_default_scenario()\nprint_scenario_info(scenario)\n\nMAX_TIME = DEFAULT_MAX_TIME\n\n# ============================================================\n# KEY CONFIG: Set to None for variable durability, or a float (e.g., 1.0) to fix it\n# ============================================================\nFIXED_DURABILITY = 1.0  # None = variable, 1.0 = fixed for debugging\nMAX_ACTION_DELAY = 100.0\n\nprint(f\"\\nDurability config: {'FIXED at ' + str(FIXED_DURABILITY) if FIXED_DURABILITY else 'VARIABLE'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare baseline policies (always uses variable durability)\nresults = compare_with_baselines(scenario, max_time=MAX_TIME)\n\nif FIXED_DURABILITY is not None:\n    print(f\"\\nNote: Baselines above use variable durability.\")\n    print(f\"      Training uses fixed durability={FIXED_DURABILITY} - see sanity check for proper baseline.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sanity check: Run optimal policy through RL environment\n# Uses same FIXED_DURABILITY as training config\nsanity_results = run_sanity_check(scenario, max_time=MAX_TIME, fixed_durability=FIXED_DURABILITY)\n\nif FIXED_DURABILITY is None:\n    print(f\"\\nExpected: Should match 'optimised_linear' baseline (~{results['optimised_linear']['mean']:.2f})\")\nelse:\n    print(f\"\\nThis is the proper baseline for fixed_durability={FIXED_DURABILITY}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train SAC\n",
    "\n",
    "SAC uses continuous action space - the agent outputs the delay directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train SAC\nimport torch.nn as nn\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.callbacks import CheckpointCallback\n\n# Environment settings (FIXED_DURABILITY and MAX_ACTION_DELAY defined in cell 3)\nREWARD_SCALE = None       # None = use failure_cost (rewards in ~[0, 2.5])\n\n# SAC hyperparameters\nLEARNING_RATE = 3e-4\nBATCH_SIZE = 256          # SAC typically uses smaller batches than DQN needed\nGAMMA = 0.99\nTAU = 0.005               # Soft update coefficient (SAC default)\nBUFFER_SIZE = 50_000\n\n# Entropy coefficient (auto-tuned by default)\nENT_COEF = 'auto'         # Or set to fixed value like 0.1\n\n# Checkpointing\nSAVE_CHECKPOINTS = True\nCHECKPOINT_FREQ = 10_000\n\n# Create training environment with CONTINUOUS action space\nenv = ServiceEnv(\n    scenario, \n    max_time=MAX_TIME, \n    seed=42,\n    reward_scale=REWARD_SCALE,\n    max_action_delay=MAX_ACTION_DELAY,\n    fixed_durability=FIXED_DURABILITY,\n    continuous_actions=True,  # <-- Key difference from DQN\n)\n\nprint(f\"Action space: {env.action_space}\")\nprint(f\"Observation space: {env.observation_space}\")\nprint(f\"Reward scale: {env.reward_scale}\")\nprint(f\"Fixed durability: {FIXED_DURABILITY}\")\n\n# Optimal interval for durability=1.0: 24.2 + 24.2*1.0 = 48.4\nif FIXED_DURABILITY is not None:\n    opt_interval = 24.2 + 24.2 * FIXED_DURABILITY\n    print(f\"Optimal interval (from linear policy): {opt_interval:.1f}\")\nelse:\n    print(\"Optimal interval varies: 24.2 + 24.2 * durability\")\n\n# Create evaluation environments\neval_env = ServiceEnv(\n    scenario, max_time=MAX_TIME, seed=123,\n    max_action_delay=MAX_ACTION_DELAY, fixed_durability=FIXED_DURABILITY,\n    continuous_actions=True,\n)\noptimal_env = ServiceEnv(\n    scenario, max_time=MAX_TIME, seed=123,\n    max_action_delay=MAX_ACTION_DELAY, fixed_durability=FIXED_DURABILITY,\n    continuous_actions=True,\n    use_optimal_policy=True,\n)\n\n# Wrap training env for logging\nenv = Monitor(env)\n\n# Network architecture\npolicy_kwargs = dict(\n    net_arch=[64, 64],\n    activation_fn=nn.ReLU,\n)\n\n# Print configuration\nprint(f\"\\nSAC Configuration:\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Soft update tau: {TAU}\")\nprint(f\"  Entropy coef: {ENT_COEF}\")\nprint(f\"  Network: {policy_kwargs}\")\n\n# TensorBoard: run `tensorboard --logdir ./logs` to watch\nprint(\"\\nTraining SAC (verbose=0, use TensorBoard to monitor)...\")\n\nmodel = SAC(\n    'MlpPolicy',\n    env,\n    learning_rate=LEARNING_RATE,\n    buffer_size=BUFFER_SIZE,\n    batch_size=BATCH_SIZE,\n    gamma=GAMMA,\n    tau=TAU,\n    ent_coef=ENT_COEF,\n    policy_kwargs=policy_kwargs,\n    verbose=0,\n    seed=42,\n    tensorboard_log=\"./logs\",\n)\n\n# Callbacks for logging\nreward_logger = RewardLoggerCallback()\ncallbacks = [\n    reward_logger,\n    ContinuousActionStatsCallback(log_freq=1000),  # For continuous actions\n    EpisodeDiagnosticsCallback(\n        max_time=MAX_TIME,\n        log_freq=100,\n        eval_env=eval_env,\n        optimal_env=optimal_env,\n        n_eval_episodes=50,\n    ),\n]\n\n# Optional: Save checkpoints during training\nif SAVE_CHECKPOINTS:\n    checkpoint_callback = CheckpointCallback(\n        save_freq=CHECKPOINT_FREQ,\n        save_path='./checkpoints/',\n        name_prefix='sac_service',\n    )\n    callbacks.append(checkpoint_callback)\n    print(f\"  Checkpoints: every {CHECKPOINT_FREQ} steps to ./checkpoints/\")\n\nmodel.learn(total_timesteps=200_000, callback=callbacks)\n\n# Store for later cells\ntraining_rewards = reward_logger.episode_rewards\ntrained_model = model\n\nprint(f\"\\nTraining complete!\")\nprint(f\"Episodes: {len(training_rewards)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "rewards = training_rewards\n",
    "window = 50\n",
    "if len(rewards) > window:\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(smoothed, label=f'Rolling avg (window={window})')\n",
    "    ax.scatter(range(len(rewards)), rewards, alpha=0.1, s=1, label='Raw')\n",
    "else:\n",
    "    ax.plot(rewards)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Cumulative Reward')\n",
    "ax.set_title('SAC Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained SAC model\n",
    "sac_rewards = evaluate_model(\n",
    "    trained_model,\n",
    "    scenario,\n",
    "    n_episodes=500,\n",
    "    max_time=MAX_TIME,\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    max_action_delay=MAX_ACTION_DELAY,\n",
    "    fixed_durability=FIXED_DURABILITY,\n",
    "    continuous_actions=True,\n",
    ")\n",
    "\n",
    "print(f\"SAC Performance (500 episodes, original metric):\")\n",
    "print(f\"  Mean reward: {np.mean(sac_rewards):.2f}\")\n",
    "print(f\"  Std reward:  {np.std(sac_rewards):.2f}\")\n",
    "print(f\"  Min reward:  {np.min(sac_rewards):.2f}\")\n",
    "print(f\"  Max reward:  {np.max(sac_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare SAC vs baselines\nfig, ax = plt.subplots(figsize=(10, 5))\n\nall_results = {name: stats['mean'] for name, stats in results.items()}\nall_results['SAC'] = np.mean(sac_rewards)\n\n# Add sanity check (optimal policy with same durability config)\nif FIXED_DURABILITY is not None:\n    all_results['optimal (fixed d)'] = sanity_results['mean_reward']\n\nsorted_results = sorted(all_results.items(), key=lambda x: x[1])\nnames = [r[0] for r in sorted_results]\nmeans = [r[1] for r in sorted_results]\n\ncolors = ['green' if n == 'SAC' else 'orange' if 'optimal' in n else 'steelblue' for n in names]\nbars = ax.barh(names, means, color=colors)\n\nax.set_xlabel('Mean Reward')\ntitle = 'SAC vs Baselines'\nif FIXED_DURABILITY is not None:\n    title += f' (fixed durability={FIXED_DURABILITY})'\nax.set_title(title)\nax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n\nfor bar, mean in zip(bars, means):\n    ax.text(mean + 5, bar.get_y() + bar.get_height()/2, \n            f'{mean:.1f}', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\nif FIXED_DURABILITY is not None:\n    print(f\"\\nNote: 'optimal (fixed d)' uses same fixed_durability={FIXED_DURABILITY} as SAC.\")\n    print(f\"      Other baselines use variable durability (not directly comparable).\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse SAC Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyse action (delay) distribution across different states\nimport pandas as pd\n\nenv = ServiceEnv(\n    scenario, max_time=MAX_TIME, seed=42,\n    max_action_delay=MAX_ACTION_DELAY,\n    fixed_durability=FIXED_DURABILITY,  # Match training config\n    continuous_actions=True,\n)\n\nstate_actions = []\nfor ep in range(100):\n    obs, _ = env.reset(seed=42 + ep)\n    done = False\n    while not done:\n        action, _ = trained_model.predict(obs, deterministic=True)\n        delay = float(action[0])  # Continuous action is the delay directly\n        state_actions.append({\n            'current_time': obs[0] * MAX_TIME,\n            'last_interval': obs[1] * MAX_TIME,\n            'service_count': obs[2] * 100,\n            'avg_interval': obs[3] * MAX_TIME,\n            'durability': obs[4] * 10,\n            'delay': delay\n        })\n        obs, _, terminated, truncated, _ = env.step(action)\n        done = terminated or truncated\n\ndf = pd.DataFrame(state_actions)\n\nprint(\"Delay statistics:\")\nprint(f\"  Mean: {df['delay'].mean():.1f}\")\nprint(f\"  Std:  {df['delay'].std():.1f}\")\nprint(f\"  Min:  {df['delay'].min():.1f}\")\nprint(f\"  Max:  {df['delay'].max():.1f}\")\nprint(f\"\\nTotal decisions: {len(df)}\")\n\n# Optimal: 24.2 + 24.2 * 1.0 = 48.4 for durability=1.0\nif FIXED_DURABILITY is not None:\n    print(f\"\\nOptimal delay for durability={FIXED_DURABILITY}: {24.2 + 24.2 * FIXED_DURABILITY:.1f}\")\nelse:\n    print(\"\\nOptimal delay varies with durability: 24.2 + 24.2 * durability\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot delay distribution and relationships\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\n# Histogram of delays\nax = axes[0]\nax.hist(df['delay'], bins=30, edgecolor='black', alpha=0.7)\nif FIXED_DURABILITY is not None:\n    opt_delay = 24.2 + 24.2 * FIXED_DURABILITY\n    ax.axvline(opt_delay, color='red', linestyle='--', label=f'Optimal ({opt_delay:.1f})')\nax.set_xlabel('Delay')\nax.set_ylabel('Count')\nax.set_title('Distribution of Chosen Delays')\nax.legend()\n\n# Delay vs durability (only meaningful if durability varies)\nax = axes[1]\nif FIXED_DURABILITY is not None:\n    # With fixed durability, show delay vs service_count instead\n    ax.scatter(df['service_count'], df['delay'], alpha=0.5, s=20)\n    ax.axhline(24.2 + 24.2 * FIXED_DURABILITY, color='red', linestyle='--', label='Optimal')\n    ax.set_xlabel('Service Count')\n    ax.set_ylabel('Chosen Delay')\n    ax.set_title(f'Delay vs Service Count (durability={FIXED_DURABILITY})')\n    ax.legend()\nelse:\n    ax.scatter(df['durability'], df['delay'], alpha=0.5, s=20)\n    # Plot optimal linear policy for reference\n    d_range = np.linspace(df['durability'].min(), df['durability'].max(), 50)\n    ax.plot(d_range, 24.2 + 24.2 * d_range, 'r--', label='Optimal linear')\n    ax.set_xlabel('Durability')\n    ax.set_ylabel('Chosen Delay')\n    ax.set_title('Delay vs Durability')\n    ax.legend()\n\n# Delay vs time\nax = axes[2]\nscatter = ax.scatter(df['current_time'], df['delay'], c=df['service_count'], alpha=0.5, s=20, cmap='viridis')\nax.set_xlabel('Current Time')\nax.set_ylabel('Chosen Delay')\nax.set_title('Delay vs Time (coloured by service count)')\nplt.colorbar(scatter, ax=ax, label='Service Count')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "model_path = '../models/sac_service_policy'\n",
    "trained_model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}