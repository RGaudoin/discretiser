{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Policy Training - SAC\n",
    "\n",
    "Train SAC (Soft Actor-Critic) with continuous action space to learn service policies.\n",
    "\n",
    "Key differences from DQN:\n",
    "- Continuous action space: agent outputs delay directly [0, max_delay]\n",
    "- Off-policy (like DQN) - efficient replay buffer usage\n",
    "- Entropy regularisation - helps exploration in noisy environments\n",
    "\n",
    "See also:\n",
    "- `rl_dqn.ipynb` - DQN with discrete action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.rl import (\n",
    "    ServiceEnv,\n",
    "    get_default_scenario,\n",
    "    print_scenario_info,\n",
    "    compare_with_baselines,\n",
    "    run_sanity_check,\n",
    "    evaluate_model,\n",
    "    RewardLoggerCallback,\n",
    "    EpisodeDiagnosticsCallback,\n",
    "    ContinuousActionStatsCallback,\n",
    "    DEFAULT_MAX_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use shared scenario setup\n",
    "scenario = get_default_scenario()\n",
    "print_scenario_info(scenario)\n",
    "\n",
    "MAX_TIME = DEFAULT_MAX_TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline policies\n",
    "results = compare_with_baselines(scenario, max_time=MAX_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Run optimal policy through RL environment\n",
    "sanity_results = run_sanity_check(scenario, max_time=MAX_TIME, fixed_durability=1.0)\n",
    "print(f\"\\nExpected: Should match 'optimised_linear' baseline (~{results['optimised_linear']['mean']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train SAC\n",
    "\n",
    "SAC uses continuous action space - the agent outputs the delay directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SAC\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# Environment settings\n",
    "MAX_ACTION_DELAY = 100.0  # Maximum delay agent can output\n",
    "REWARD_SCALE = None       # None = use failure_cost (rewards in ~[0, 2.5])\n",
    "FIXED_DURABILITY = 1.0    # Fix durability for debugging (None = use generated)\n",
    "\n",
    "# SAC hyperparameters\n",
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 256          # SAC typically uses smaller batches than DQN needed\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005               # Soft update coefficient (SAC default)\n",
    "BUFFER_SIZE = 50_000\n",
    "\n",
    "# Entropy coefficient (auto-tuned by default)\n",
    "ENT_COEF = 'auto'         # Or set to fixed value like 0.1\n",
    "\n",
    "# Checkpointing\n",
    "SAVE_CHECKPOINTS = True\n",
    "CHECKPOINT_FREQ = 10_000\n",
    "\n",
    "# Create training environment with CONTINUOUS action space\n",
    "env = ServiceEnv(\n",
    "    scenario, \n",
    "    max_time=MAX_TIME, \n",
    "    seed=42,\n",
    "    reward_scale=REWARD_SCALE,\n",
    "    max_action_delay=MAX_ACTION_DELAY,\n",
    "    fixed_durability=FIXED_DURABILITY,\n",
    "    continuous_actions=True,  # <-- Key difference from DQN\n",
    ")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Reward scale: {env.reward_scale}\")\n",
    "print(f\"Fixed durability: {FIXED_DURABILITY}\")\n",
    "\n",
    "# Optimal interval for durability=1.0: 24.2 + 24.2*1.0 = 48.4\n",
    "if FIXED_DURABILITY is not None:\n",
    "    opt_interval = 24.2 + 24.2 * FIXED_DURABILITY\n",
    "    print(f\"Optimal interval (from linear policy): {opt_interval:.1f}\")\n",
    "\n",
    "# Create evaluation environments\n",
    "eval_env = ServiceEnv(\n",
    "    scenario, max_time=MAX_TIME, seed=123,\n",
    "    max_action_delay=MAX_ACTION_DELAY, fixed_durability=FIXED_DURABILITY,\n",
    "    continuous_actions=True,\n",
    ")\n",
    "optimal_env = ServiceEnv(\n",
    "    scenario, max_time=MAX_TIME, seed=123,\n",
    "    max_action_delay=MAX_ACTION_DELAY, fixed_durability=FIXED_DURABILITY,\n",
    "    continuous_actions=True,\n",
    "    use_optimal_policy=True,\n",
    ")\n",
    "\n",
    "# Wrap training env for logging\n",
    "env = Monitor(env)\n",
    "\n",
    "# Network architecture\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[64, 64],\n",
    "    activation_fn=nn.ReLU,\n",
    ")\n",
    "\n",
    "# Print configuration\n",
    "print(f\"\\nSAC Configuration:\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Soft update tau: {TAU}\")\n",
    "print(f\"  Entropy coef: {ENT_COEF}\")\n",
    "print(f\"  Network: {policy_kwargs}\")\n",
    "\n",
    "# TensorBoard: run `tensorboard --logdir ./logs` to watch\n",
    "print(\"\\nTraining SAC (verbose=0, use TensorBoard to monitor)...\")\n",
    "\n",
    "model = SAC(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    gamma=GAMMA,\n",
    "    tau=TAU,\n",
    "    ent_coef=ENT_COEF,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=0,\n",
    "    seed=42,\n",
    "    tensorboard_log=\"./logs\",\n",
    ")\n",
    "\n",
    "# Callbacks for logging\n",
    "reward_logger = RewardLoggerCallback()\n",
    "callbacks = [\n",
    "    reward_logger,\n",
    "    ContinuousActionStatsCallback(log_freq=1000),  # For continuous actions\n",
    "    EpisodeDiagnosticsCallback(\n",
    "        max_time=MAX_TIME,\n",
    "        log_freq=100,\n",
    "        eval_env=eval_env,\n",
    "        optimal_env=optimal_env,\n",
    "        n_eval_episodes=50,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Optional: Save checkpoints during training\n",
    "if SAVE_CHECKPOINTS:\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=CHECKPOINT_FREQ,\n",
    "        save_path='./checkpoints/',\n",
    "        name_prefix='sac_service',\n",
    "    )\n",
    "    callbacks.append(checkpoint_callback)\n",
    "    print(f\"  Checkpoints: every {CHECKPOINT_FREQ} steps to ./checkpoints/\")\n",
    "\n",
    "model.learn(total_timesteps=200_000, callback=callbacks)\n",
    "\n",
    "# Store for later cells\n",
    "training_rewards = reward_logger.episode_rewards\n",
    "trained_model = model\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Episodes: {len(training_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "rewards = training_rewards\n",
    "window = 50\n",
    "if len(rewards) > window:\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(smoothed, label=f'Rolling avg (window={window})')\n",
    "    ax.scatter(range(len(rewards)), rewards, alpha=0.1, s=1, label='Raw')\n",
    "else:\n",
    "    ax.plot(rewards)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Cumulative Reward')\n",
    "ax.set_title('SAC Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained SAC model\n",
    "sac_rewards = evaluate_model(\n",
    "    trained_model,\n",
    "    scenario,\n",
    "    n_episodes=500,\n",
    "    max_time=MAX_TIME,\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    max_action_delay=MAX_ACTION_DELAY,\n",
    "    fixed_durability=FIXED_DURABILITY,\n",
    "    continuous_actions=True,\n",
    ")\n",
    "\n",
    "print(f\"SAC Performance (500 episodes, original metric):\")\n",
    "print(f\"  Mean reward: {np.mean(sac_rewards):.2f}\")\n",
    "print(f\"  Std reward:  {np.std(sac_rewards):.2f}\")\n",
    "print(f\"  Min reward:  {np.min(sac_rewards):.2f}\")\n",
    "print(f\"  Max reward:  {np.max(sac_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SAC vs baselines\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "all_results = {name: stats['mean'] for name, stats in results.items()}\n",
    "all_results['SAC'] = np.mean(sac_rewards)\n",
    "\n",
    "sorted_results = sorted(all_results.items(), key=lambda x: x[1])\n",
    "names = [r[0] for r in sorted_results]\n",
    "means = [r[1] for r in sorted_results]\n",
    "\n",
    "colors = ['green' if n == 'SAC' else 'steelblue' for n in names]\n",
    "bars = ax.barh(names, means, color=colors)\n",
    "\n",
    "ax.set_xlabel('Mean Reward')\n",
    "ax.set_title('SAC vs Baseline Policies')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "for bar, mean in zip(bars, means):\n",
    "    ax.text(mean + 5, bar.get_y() + bar.get_height()/2, \n",
    "            f'{mean:.1f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse SAC Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse action (delay) distribution across different states\n",
    "import pandas as pd\n",
    "\n",
    "env = ServiceEnv(\n",
    "    scenario, max_time=MAX_TIME, seed=42,\n",
    "    max_action_delay=MAX_ACTION_DELAY,\n",
    "    continuous_actions=True,\n",
    ")\n",
    "\n",
    "state_actions = []\n",
    "for ep in range(100):\n",
    "    obs, _ = env.reset(seed=42 + ep)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = trained_model.predict(obs, deterministic=True)\n",
    "        delay = float(action[0])  # Continuous action is the delay directly\n",
    "        state_actions.append({\n",
    "            'current_time': obs[0] * MAX_TIME,\n",
    "            'last_interval': obs[1] * MAX_TIME,\n",
    "            'service_count': obs[2] * 100,\n",
    "            'avg_interval': obs[3] * MAX_TIME,\n",
    "            'durability': obs[4] * 10,\n",
    "            'delay': delay\n",
    "        })\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "df = pd.DataFrame(state_actions)\n",
    "\n",
    "print(\"Delay statistics:\")\n",
    "print(f\"  Mean: {df['delay'].mean():.1f}\")\n",
    "print(f\"  Std:  {df['delay'].std():.1f}\")\n",
    "print(f\"  Min:  {df['delay'].min():.1f}\")\n",
    "print(f\"  Max:  {df['delay'].max():.1f}\")\n",
    "print(f\"\\nTotal decisions: {len(df)}\")\n",
    "\n",
    "# Optimal: 24.2 + 24.2 * 1.0 = 48.4 for durability=1.0\n",
    "print(f\"\\nOptimal delay for durability=1.0: 48.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot delay distribution and relationships\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Histogram of delays\n",
    "ax = axes[0]\n",
    "ax.hist(df['delay'], bins=30, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(48.4, color='red', linestyle='--', label='Optimal (48.4)')\n",
    "ax.set_xlabel('Delay')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Chosen Delays')\n",
    "ax.legend()\n",
    "\n",
    "# Delay vs durability\n",
    "ax = axes[1]\n",
    "ax.scatter(df['durability'], df['delay'], alpha=0.5, s=20)\n",
    "# Plot optimal linear policy for reference\n",
    "d_range = np.linspace(df['durability'].min(), df['durability'].max(), 50)\n",
    "ax.plot(d_range, 24.2 + 24.2 * d_range, 'r--', label='Optimal linear')\n",
    "ax.set_xlabel('Durability')\n",
    "ax.set_ylabel('Chosen Delay')\n",
    "ax.set_title('Delay vs Durability')\n",
    "ax.legend()\n",
    "\n",
    "# Delay vs time\n",
    "ax = axes[2]\n",
    "scatter = ax.scatter(df['current_time'], df['delay'], c=df['durability'], alpha=0.5, s=20, cmap='viridis')\n",
    "ax.set_xlabel('Current Time')\n",
    "ax.set_ylabel('Chosen Delay')\n",
    "ax.set_title('Delay vs Time (coloured by durability)')\n",
    "plt.colorbar(scatter, ax=ax, label='Durability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "model_path = '../models/sac_service_policy'\n",
    "trained_model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
